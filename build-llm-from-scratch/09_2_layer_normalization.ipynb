{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5) #A\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Mean:  tensor([-0.3596, -0.2606]) \n",
      "Input var:  tensor([0.2518, 0.3342])\n",
      "\n",
      "Output Mean:  tensor([0.1324, 0.2170], grad_fn=<MeanBackward1>) \n",
      "Output var:  tensor([0.0231, 0.0398], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('Input Mean: ', batch_example.mean(dim=-1),  '\\nInput var: ', batch_example.var(dim=-1))\n",
    "print('\\nOutput Mean: ', out.mean(dim=-1), '\\nOutput var: ', out.var(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using keepdim=True in operations like mean or variance calculation ensures that the output tensor retains the same number of dimensions as the input tensor, even though the operation reduces the tensor along the dimension specified via dim.\n",
    "\n",
    "For instance, without keepdim=True, the returned mean tensor would be a 2-dimensional vector [0.1324, 0.2170] instead of a 2Ã—1-dimensional matrix [[0.1324], [0.2170]].\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "\n",
    "For a 2D tensor (like a matrix), using dim=-1 for operations such as mean or variance calculation is the same as using dim=1.\n",
    "\n",
    "This is because -1 refers to the tensor's last dimension, which corresponds to the columns in a 2D tensor.\n",
    "\n",
    "Later, when adding layer normalization to the GPT model, which produces 3D tensors with shape [batch_size, num_tokens, embedding_size], we can still use dim=-1 for normalization across the last dimension, avoiding a change from dim=1 to dim=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized output Mean:  tensor([-5.9605e-08,  1.9868e-08], grad_fn=<MeanBackward1>) \n",
      "Normalized output var:  tensor([1.0000, 1.0000], grad_fn=<VarBackward0>)\n",
      "\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=1, keepdim=True)\n",
    "var = out.var(dim=1, keepdim=True)\n",
    "normalized_out = (out - mean) / torch.sqrt(var)\n",
    "print('\\nNormalized output Mean: ', normalized_out.mean(dim=-1),\n",
    "      '\\nNormalized output var: ', normalized_out.var(dim=-1))\n",
    "\n",
    "print('\\n', normalized_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specific implementation of layer Normalization operates on the last dimension of the input tensor x, which represents the embedding dimension (emb_dim).\n",
    "\n",
    "The variable eps is a small constant (epsilon) added to the variance to prevent division by zero during normalization.\n",
    "\n",
    "The scale and shift are two trainable parameters (of the same dimension as the input) that the LLM automatically adjusts during training if it is determined that doing so would improve the model's performance on its training task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our variance calculation method, we have opted for an implementation detail by setting unbiased=False.\n",
    "\n",
    "For those curious about what this means, in the variance calculation, we divide by the number of inputs n in the variance formula.\n",
    "\n",
    "This approach does not apply Bessel's correction, which typically uses n-1 instead of n in the denominator to adjust for bias in sample variance estimation.\n",
    "\n",
    "This decision results in a so-called biased estimate of the variance.\n",
    "\n",
    "For large-scale language models (LLMs), where the embedding dimension n is significantly large, the difference between using n and n-1 is practically negligible.\n",
    "\n",
    "We chose this approach to ensure compatibility with the GPT-2 model's normalization layers and because it reflects TensorFlow's default behavior, which was used to implement the original GPT2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying LayerNorm Class on BatchInput Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[-2.9802e-08],\n",
      "        [ 0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.2499],\n",
      "        [1.2500]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, keepdim=True)\n",
    "\n",
    "print('Mean:\\n', mean)\n",
    "print('Variance:\\n', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
